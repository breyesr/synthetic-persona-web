# Technical Handover: RAG Pipeline & Vector Database

This document provides a complete technical overview of the new Retrieval-Augmented Generation (RAG) architecture, development setup, and maintenance procedures.

## 1. Architecture & Data Flow

The new architecture is designed to be scalable and efficient, moving from a slow, file-based system to a robust pipeline centered around a Postgres vector database.

### Data Pipeline Overview

The flow of data from raw documents to the LLM is as follows:

1.  **Source Documents**: JSON persona definitions are stored in the `/data/personas` directory. (Phase 2 will add support for PDFs).
2.  **Ingestion & Embedding**: The `npm run embed` script is executed locally.
    *   It finds all relevant source documents.
    *   It chunks the text content into smaller, manageable pieces.
    *   It calls the OpenAI Embeddings API (`text-embedding-3-small`) for each chunk to create a vector.
    *   It stores each chunk's text, its vector, and relevant metadata in the `documents` table in the Postgres database.
3.  **User Query**: A user submits a query through the application frontend.
4.  **Hybrid Search**: The API receives the query and performs a hybrid search against the `documents` table in the Postgres database (`pgvector`).
    *   **Keyword Search**: A full-text search finds chunks with exact keyword matches.
    *   **Vector Search**: A vector similarity search finds chunks that are semantically related to the query.
    *   **Re-ranking**: The results from both searches are combined and re-ranked to provide the most relevant context.
5.  **LLM Prompt & Response**: The retrieved context chunks are injected into a system prompt, combined with the user's original query, and sent to the OpenAI Chat API (`gpt-4o-mini`). The response is then streamed back to the user.

### Postgres Schema Reference

All indexed content is stored in a single table named `documents`.

| Column             | Type             | Description                                                                                                                              |
| ------------------ | ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------- |
| `id`               | `UUID`           | **Primary Key**. A unique, deterministically generated ID for each chunk.                                                                  |
| `content`          | `TEXT`           | The raw text content of the chunk.                                                                                                       |
| `embedding`        | `VECTOR(1536)`   | The 1536-dimension vector embedding of the `content`, generated by OpenAI. Indexed with HNSW for fast similarity search.                 |
| `metadata`         | `JSONB`          | A JSON object containing metadata about the chunk. Indexed with GIN for fast filtering. **Example:** `{ "source_file": "data/personas/fisioterapeuta.json", "persona_id": "fisioterapeuta" }`. |
| `content_tsvector` | `TSVECTOR`       | A pre-computed vector of the `content` for fast full-text search. Indexed with GIN.                                                      |

The `metadata` column is critical for filtering. For example, all queries are filtered by `persona_id` *before* the search executes, ensuring the LLM never gets context from the wrong persona.

## 2. "Zero-to-Hero" Setup Guide

This guide assumes you are on a new machine with **Node.js**, **npm**, and **Docker Desktop** already installed.

#### 1. Install Dependencies

Clone the repository and install the required npm packages.
```bash
git clone https://github.com/breyesr/synthetic-persona-web.git
cd synthetic-persona-web
npm install
```

#### 2. Configure Environment Variables

Create a local environment file by copying the example.
```bash
cp .env.example .env.local
```
Now, edit the `.env.local` file and add your keys.

| Key                  | Description                                                                    | Where to Get It                                                                    |
| -------------------- | ------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------- |
| `POSTGRES_URL_LOCAL` | The connection string for the local Docker database.                           | Pre-configured in `.env.example`. No changes needed.                               |
| `POSTGRES_URL`       | The connection string for the production Vercel/Neon database.                 | Automatically set by Vercel in deployment environments. Not needed for local setup. |
| `OPENAI_API_KEY`     | Your secret key for the OpenAI API.                                            | [platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys) |
| `LLAMA_CLOUD_API_KEY`| Your secret key for LlamaParse. **(Needed for Phase 2 PDF Ingestion)**.         | [cloud.llamaindex.ai](https://cloud.llamaindex.ai/)                                 |

#### 3. Start the Local Database

With Docker Desktop running, start the Postgres database container.
```bash
docker-compose up -d
```
This will start a database on port `5433` that persists between restarts.

#### 4. Set Up Database Schema

Create the `documents` table and all necessary indexes in the local database.
```bash
npm run db:setup
```

#### 5. Seed the Database (Ingestion)

Process the files in the `/data` directory, generate embeddings, and load them into your local database.
```bash
npm run embed
```

#### 6. Start the Application

Run the Next.js development server.
```bash
npm run dev
```
The application will be available at `http://localhost:3000` (or the next available port).

## 3. Ingestion Runbook

This runbook explains how to manage the content in the vector store.

#### How do I add new content?

Place the new JSON (or in Phase 2, PDF) files into the appropriate subdirectory within the `/data` directory.

#### What command do I run to index them?

After adding or modifying files, run the `embed` script from your terminal:
```bash
npm run embed
```
The script will automatically find the new files, chunk and embed them, and add them to the database.

#### How do I delete an old document?

Simply **delete the source file** from the `/data` directory.

The next time you run `npm run embed`, the script will notice that the file is gone and will automatically delete all chunks associated with that file from the database. This "upsert" and cleanup logic ensures the database is always in sync with the contents of the `/data` directory.

## 4. Deployment & Troubleshooting

#### Vercel Configuration

-   **Build Command**: `npm run build` (This is the standard Next.js command).
-   **Output Directory**: `.next` (Standard for Next.js).
-   **Environment Variables**: The production `POSTGRES_URL` and `OPENAI_API_KEY` must be set in the Vercel project settings for the "Production" and "Preview" environments. The `POSTGRES_URL` is automatically set when you integrate a Vercel Postgres/Neon database.

#### Common Errors & FAQ

-   **Error: `ECONNREFUSED` when running the app.**
    *   **Meaning**: The application cannot connect to the database.
    *   **Fix**: Your local Docker container is likely not running. Run `docker-compose up -d` in your terminal.

-   **Error: `401 Incorrect API key` from OpenAI.**
    *   **Meaning**: The `OPENAI_API_KEY` in your `.env.local` file is invalid or missing.
    *   **Fix**: Go to your OpenAI account, generate a new, valid API key, and place it in your `.env.local` file.

-   **Error: `Database connection string is not set` when running scripts.**
    *   **Meaning**: The `npm run` command is not picking up your `.env.local` file correctly.
    *   **Fix**: Ensure your `.env.local` file is in the root of the project and correctly formatted. As a last resort, you can pass the variables directly: `POSTGRES_URL_LOCAL="..." npm run db:setup`.

-   **Question: The search returns 0 results.**
    *   **Meaning**: The database is likely empty.
    *   **Fix**: Make sure you have successfully run the `npm run embed` command to populate the database with data.
